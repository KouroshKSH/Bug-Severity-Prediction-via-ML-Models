# -*- coding: utf-8 -*-
"""CS412_HW4_Finetuning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JLCWWFkPA8boTSdVTqnfKGNLgPox7a0_

# Info
## Group Name
KAKO

## Group Members:
- Kourosh Sharifi
- Arya Hassibi
- Kutluhan Ayguzel
- Ozan Parlayan
"""

!pip install torch==2.0.1
!pip install transformers==4.32.1
!pip install datasets==2.14.4
!pip install peft==0.5.0
!pip install bitsandbytes==0.41.1
!pip install trl==0.7.1

import json
import re
from tqdm import tqdm
from pprint import pprint

import pandas as pd
import torch
from datasets import Dataset, load_dataset
from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    TrainingArguments,
)
from trl import SFTTrainer

OUTPUT_DIR = 'experiments'
DEVICE = "cuda:0" if torch.cuda.is_available() else "cpu"
MODEL_NAME = "NousResearch/Llama-2-7b-hf"

"""### Data Loading & Preprocessing"""

DEFAULT_SYSTEM_PROMPT = "Below is an instruction that describes a task. Write a response that appropriately completes the request"

def format_prompt(query, response, system_prompt = DEFAULT_SYSTEM_PROMPT):
  return f"""###Instruction:
{system_prompt}
### Input:
{query}
### Response:
{response}""" + " </s>"

def generate_alpaca_prompt(example):
    return {
        'text' : format_prompt(example['instruction'], example['output'])
    }

# Read data from csv
dataset = pd.read_csv('CS412_HW4_Step1_Data.csv')
dataset = dataset[['instruction', 'output']]

# Convert the dataset into Huggingface Dataset
dataset = Dataset.from_pandas(dataset)

# Apply Alpaca format to instruction-response pairs
dataset = dataset.map(generate_alpaca_prompt).rename_column('output', 'response')
dataset

"""### Loading Model & Tokenizer (4 Bit Quantized)"""

def get_quantized_model_and_tokenizer(model_id):
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16,
    )

    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        use_safetensors=True,
        quantization_config=bnb_config,
        trust_remote_code=True,
        device_map="auto",
    )

    tokenizer = AutoTokenizer.from_pretrained(model_id)
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "right"
    return model, tokenizer

model, tokenizer = get_quantized_model_and_tokenizer(MODEL_NAME)

model.config.use_cache = False
model.gradient_checkpointing_enable()

model = prepare_model_for_kbit_training(model)

"""## Training"""

### LoRA Configuration
lora_r = 16
lora_alpha = 32
lora_dropout = 0.05
lora_target_modules = ["q_proj","v_proj"]

peft_config = LoraConfig(
    r=lora_r,
    lora_alpha=lora_alpha,
    lora_dropout=lora_dropout,
    target_modules=lora_target_modules,
    bias="none",
    task_type="CAUSAL_LM",
)

### Training Arguments
OUTPUT_DIR = "experiments"

training_arguments = TrainingArguments(
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    gradient_accumulation_steps=1,
    optim="paged_adamw_32bit",
    learning_rate=2e-4,
    fp16=True,
    max_grad_norm=0.3,
    weight_decay=0.001,
    num_train_epochs=2,
    warmup_ratio=0.03,
    save_strategy="no",
    group_by_length=True,
    output_dir=OUTPUT_DIR,
    logging_strategy='steps',
    logging_steps=0.1,
    save_safetensors=True,
    lr_scheduler_type="constant",
    seed=0,
)

### Trainer Initialization
trainer = SFTTrainer(
    model=model,
    train_dataset=dataset,
    peft_config=peft_config,
    dataset_text_field="text",
    max_seq_length=1024,
    tokenizer=tokenizer,
    args=training_arguments,
)

# Finetuning LLama2
trainer.train()

# Saving model
trainer.save_model()

"""### Inference (Restart the Notebook before running this part)

* **Runtime -> Restart Session**
"""

import json
import re
from tqdm import tqdm
from pprint import pprint

import pandas as pd
import torch
from datasets import Dataset, load_dataset
from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    TrainingArguments,
)
from trl import SFTTrainer

OUTPUT_DIR = 'experiments'
DEVICE = "cuda:0" if torch.cuda.is_available() else "cpu"
MODEL_NAME = "NousResearch/Llama-2-7b-hf"

"""#### Loading Finetuned Model & Merging the Adapters"""

# Loading base model (not quantized)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    torch_dtype=torch.float16,
    load_in_4bit=False,
    use_safetensors=True,
    trust_remote_code=True,
    device_map="auto"
)

# Loading the adapter
model = PeftModel.from_pretrained(
    model,
    OUTPUT_DIR,
    device_map='auto'
)

model = model.merge_and_unload()

tokenizer = AutoTokenizer.from_pretrained(OUTPUT_DIR)

from transformers import GenerationConfig

DEFAULT_SYSTEM_PROMPT = "Below is an instruction that describes a task. Write a response that appropriately completes the request"

def format_prompt_inference(query, system_prompt = DEFAULT_SYSTEM_PROMPT):
  return f"###Instruction:\n{system_prompt}\n\n### Input:\n{query}\n\n### Response:\n".strip()


def llama_inference(model, tokenizer, text):
  prompt = format_prompt_inference(text)
  inputs = tokenizer(prompt, return_tensors="pt").to("cuda:0" if torch.cuda.is_available() else "cpu")
  inputs_length = len(inputs["input_ids"][0])
  generation_config = GenerationConfig(
      penalty_alpha=0.6,
      do_sample=True,
      top_k=5,
      temperature=0.0001,
      repetition_penalty=1.2,
      max_new_tokens=256,
      pad_token_id=tokenizer.eos_token_id
  )
  with torch.inference_mode():
      outputs = model.generate(**inputs, generation_config=generation_config)

  return tokenizer.decode(outputs[0][inputs_length:], skip_special_tokens=True)


def save_predictions_to_csv(model, tokenizer, dataset, output_file):
    predictions = []

    for row in tqdm(dataset):
        prediction = llama_inference(model, tokenizer, row['instruction'])
        predictions.append({'Instruction': row['instruction'], 'Prediction': prediction})

    predictions_df = pd.DataFrame(predictions)
    predictions_df.to_csv(output_file, index=False)
    print('Predictions saved as csv')

# Read data from csv
dataset = pd.read_csv('CS412_HW4_Test_Data.csv')
dataset = dataset[['instruction']]

# Convert the dataset into Huggingface Dataset
dataset = Dataset.from_pandas(dataset)

# Inference and saving the predictions
save_predictions_to_csv(model, tokenizer, dataset, output_file = 'CS412_HW4_Step2_Predictions.csv')

